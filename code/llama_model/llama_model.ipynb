{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f80da35",
      "metadata": {
        "id": "6f80da35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "#secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9lBxILNiVzti",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lBxILNiVzti",
        "outputId": "2ae95f26-d8d7-47b2-9967-89afa91ec1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_all = pd.read_csv('/content/drive/My Drive/goemotions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "76bfc81e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76bfc81e",
        "outputId": "ab8dcccd-39bf-46a5-a8e2-bcc651a83d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text       id  \\\n",
            "0                                    That game hurt.  eew5j0j   \n",
            "1   >sexuality shouldn‚Äôt be a grouping category I...  eemcysk   \n",
            "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
            "3                                 Man I love reddit.  eeibobj   \n",
            "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
            "\n",
            "                author            subreddit    link_id   parent_id  \\\n",
            "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
            "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
            "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
            "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
            "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
            "\n",
            "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
            "0  1.548381e+09         1                 False           0  ...     0   \n",
            "1  1.548084e+09        37                  True           0  ...     0   \n",
            "2  1.546428e+09        37                 False           0  ...     0   \n",
            "3  1.547965e+09        18                 False           0  ...     1   \n",
            "4  1.546669e+09         2                 False           0  ...     0   \n",
            "\n",
            "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
            "0            0         0      0            0       0        0        1   \n",
            "1            0         0      0            0       0        0        0   \n",
            "2            0         0      0            0       0        0        0   \n",
            "3            0         0      0            0       0        0        0   \n",
            "4            0         0      0            0       0        0        0   \n",
            "\n",
            "   surprise  neutral  \n",
            "0         0        0  \n",
            "1         0        0  \n",
            "2         0        1  \n",
            "3         0        0  \n",
            "4         0        1  \n",
            "\n",
            "[5 rows x 37 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "de03e0f5",
      "metadata": {
        "id": "de03e0f5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U transformers datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5c629c86",
      "metadata": {
        "id": "5c629c86"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import set_seed\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "pretrained_model_name = \"google/gemma-2-2b\"\n",
        "instruction_tuned_model_name = \"google/gemma-2-2b-it\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4ed30e3a",
      "metadata": {
        "id": "4ed30e3a"
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(\n",
        "               model_name,\n",
        "               model_kwargs={'quantization_config':quantization_config if torch.cuda.is_available() else None,\n",
        "                             'max_length': 1024,\n",
        "                             'torch_dtype':torch.float16,\n",
        "                             'device_map': 'auto'}\n",
        "               ):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = 'left'\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      **model_kwargs,\n",
        "  )\n",
        "\n",
        "  return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7c669393",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c03aca2eae0d4d76beb69edaac35b53a",
            "1350df1a74c4425799d5c52adc0e0df2",
            "cb537eca95e54e1b848c0250504a5fc7",
            "879cc177d2f64f25bac81de6efa88266",
            "f5c0c20c15864f8a9069ef940f2617a1",
            "8f1f4bde4b844b43b51f7e348fa8c477",
            "6b4c202d9f9049c1b22791ff4c136aad",
            "0dbf214de76844fb98cefb64ab9f2fc7",
            "5e563994ff7d43c6b836d371086e2f78",
            "dd4b26addd5349e6895b0ad80fce18ce",
            "ba8b928255ee421291f6493246087e8c",
            "4dda80dd9d714df188ff3d50e26f7642",
            "0fc1dc62bfcb4ad2b1ecf140098d02cc",
            "cc60bcb5a3a0454cbd227873402701a0",
            "57293a7cdd56452eb5b27c92fb524604",
            "fe9e272bf51a454bb04b360460b0db07",
            "c300774212f844d78aca57f40c9bc4b2",
            "248575939d6040dea530de77d88c3dc8",
            "259da2afa42946728645391aa6d714c7",
            "e1bf2697659c438f9e1cc8818bd287dd",
            "42bb081167504bedac17e611477f4945",
            "9e74dbd4ec8d4ae4975c77f5c2f4ec9e"
          ]
        },
        "id": "7c669393",
        "outputId": "4432b557-251f-45d9-8afe-9ca2a6cf91c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c03aca2eae0d4d76beb69edaac35b53a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dda80dd9d714df188ff3d50e26f7642",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_pretrained, tokenizer_pretrained = load_model_and_tokenizer(pretrained_model_name)\n",
        "model_instruction_tuned, tokenizer_instruction_tuned = load_model_and_tokenizer(instruction_tuned_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e5f2830c",
      "metadata": {
        "id": "e5f2830c"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_model(prompt,\n",
        "              model,\n",
        "              tokenizer,\n",
        "              apply_chat_template=False,\n",
        "              generation_kwargs={\n",
        "                  'min_new_tokens': 1,\n",
        "                  'max_new_tokens': 25,\n",
        "                  'temperature': 0.1,\n",
        "                  'top_p': 0.95,\n",
        "                  'do_sample': True,\n",
        "              }\n",
        "              ):\n",
        "  set_seed(42)\n",
        "\n",
        "  if apply_chat_template:\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        [{'role': 'user', 'content': prompt}],\n",
        "        tokenize=False\n",
        "    )\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "  output_ids = model.generate(\n",
        "      **inputs,\n",
        "      **generation_kwargs\n",
        "  )\n",
        "\n",
        "  input_ids = inputs['input_ids'][0]\n",
        "  input_len = (input_ids != tokenizer.pad_token_id).sum().item()\n",
        "  tokens = output_ids[0][input_len:]\n",
        "  output = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "  output = output.strip().split(\"\\n\")[0]\n",
        "\n",
        "  return output\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_model_batch(prompts,\n",
        "              model,\n",
        "              tokenizer,\n",
        "              apply_chat_template=False,\n",
        "              generation_kwargs={\n",
        "                  'min_new_tokens': 1,\n",
        "                  'max_new_tokens': 25,\n",
        "                  'temperature': 0.1,\n",
        "                  'top_p': 0.95,\n",
        "                  'do_sample': True\n",
        "              },\n",
        "              tokenizer_kwargs={\n",
        "                  'padding':'longest',\n",
        "              }\n",
        "            ):\n",
        "  set_seed(42)\n",
        "\n",
        "  if apply_chat_template:\n",
        "    prompts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "          [{'role': 'user', 'content': prompt}],\n",
        "          tokenize=False,\n",
        "        ) for prompt in prompts\n",
        "    ]\n",
        "\n",
        "  inputs = tokenizer(prompts, return_tensors='pt', **tokenizer_kwargs).to(model.device)\n",
        "  output_ids = model.generate(\n",
        "      **inputs,\n",
        "      **generation_kwargs\n",
        "  )\n",
        "\n",
        "  batch_output = []\n",
        "  for i in range(len(prompts)):\n",
        "    input_ids = inputs['input_ids'][i]\n",
        "    input_len = (input_ids != tokenizer.pad_token_id).sum().item()\n",
        "    tokens = output_ids[i][input_len:]\n",
        "\n",
        "    text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    text = text.strip().split(\"\\n\")[0]\n",
        "\n",
        "    batch_output.append(text)\n",
        "\n",
        "  return batch_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b4ae45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44b4ae45",
        "outputId": "94b5b757-89ce-4435-f6bc-0765693b0035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing emotion classification on 500 samples...\n",
            "\n",
            "Model Output: relief\n",
            "Text: I will tell my NP he may have inspired pancakes in someone's future :)\n",
            "0: Predicted: relief, Actual: neutral\n",
            "\n",
            "Model Output: confusion\n",
            "Text: What? I didn‚Äôt understand a thing of what you said.\n",
            "1: Predicted: confusion, Actual: confusion\n",
            "\n",
            "Model Output: disgust\n",
            "Text: *Just look at the opposing team's hooker and lick your lips as every scrum starts.*\n",
            "2: Predicted: disgust, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: Meanwhile... [NAME], [NAME], [NAME], [NAME], [NAME], [NAME], [NAME], [NAME] and [NAME] don't benefit at all from playing with better players\n",
            "3: Predicted: anger, Actual: realization\n",
            "\n",
            "Model Output: anger\n",
            "Text: Taking this way farther than needed...\n",
            "4: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: what i imagine you sounded like when that cursor started freaking out\n",
            "5: Predicted: anger, Actual: annoyance\n",
            "\n",
            "Model Output: anger\n",
            "Text: If anything makes me belive someone DOESN'T get laid, it's comments like this.\n",
            "6: Predicted: anger, Actual: surprise\n",
            "\n",
            "Model Output: relief\n",
            "Text: Congratulations on your sobriety! A years worth of sobriety might be why the rum is gone üòâ I appreciate the suggestion.\n",
            "7: Predicted: relief, Actual: admiration\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: I really wanna see how this argument goes....!\n",
            "8: Predicted: curiosity, Actual: excitement\n",
            "\n",
            "Model Output: admiration\n",
            "Text: I loved coming across this today! Nice work!!\n",
            "9: Predicted: admiration, Actual: love\n",
            "\n",
            "Model Output: relief\n",
            "Text: Best way to ship an item that is roughly 120\" x 8\" x 8\" and weighs 45 lbs? Thank you!!\n",
            "10: Predicted: relief, Actual: gratitude\n",
            "\n",
            "Model Output: relief\n",
            "Text: Well it obviously helps you rationalize your total unwillingness to take action to make the world a better place. I hope that you grow past that.\n",
            "11: Predicted: relief, Actual: approval\n",
            "\n",
            "Model Output: optimism\n",
            "Text: A radical departure from the lunacy we have now? Yes.\n",
            "12: Predicted: optimism, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: And spending extravagant amounts on campaigning to impress the poor fools with the money that could have been used to improve their lives.\n",
            "13: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: ‚ÄúBut officer, I have a doctor‚Äôs note saying I *have* to expose myself to them!‚Äù\n",
            "14: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: amusement\n",
            "Text: I‚Äôm up here too man, having a great time so far!\n",
            "15: Predicted: amusement, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: That is so beautiful. It makes my heart hurt that humans can be cruel to animals when they're aware of having so much emotion.\n",
            "16: Predicted: sadness, Actual: disgust\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: I heard of people doing that in the dorms freshman year and hadn't thought of it since. People said it worked.\n",
            "17: Predicted: curiosity, Actual: optimism\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: [NAME] is also a Vikings fan. I'd love to have him on the longboat.\n",
            "18: Predicted: curiosity, Actual: neutral\n",
            "\n",
            "Model Output: disgust\n",
            "Text: >recommending Elite mode to first time players My stomach turns in disgust\n",
            "19: Predicted: disgust, Actual: disgust\n",
            "\n",
            "Model Output: confusion\n",
            "Text: Hate to be that guy, but that's not DDR, that's Pump It Up, the Korean DDR rip-off.\n",
            "20: Predicted: confusion, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: There should be a vigilante whose superpower is not having to stumble over to somebody after being injured.\n",
            "21: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: confusion\n",
            "Text: Mexico's or USA?\n",
            "22: Predicted: confusion, Actual: neutral\n",
            "\n",
            "Model Output: relief\n",
            "Text: Haha i have 2 chinchillas from a manic episode about 8 years ago. They're sweet little things.\n",
            "23: Predicted: relief, Actual: neutral\n",
            "\n",
            "Model Output: relief\n",
            "Text: I got you.\n",
            "24: Predicted: relief, Actual: caring\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Still new to me\n",
            "25: Predicted: curiosity, Actual: excitement\n",
            "\n",
            "Model Output: amusement\n",
            "Text: Great headline though\n",
            "26: Predicted: amusement, Actual: admiration\n",
            "\n",
            "Model Output: anger\n",
            "Text: If I have to watch Canada lose in a shootout I'm gonna lose it\n",
            "27: Predicted: anger, Actual: disappointment\n",
            "\n",
            "Model Output: anger\n",
            "Text: except the man with the drum APPROACHED THEM! seriously maybe if they hit him with a bike lock the way [NAME] did they wouldnt be covered\n",
            "28: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: confusion\n",
            "Text: Sorry ideologically yeah. I meant morally.\n",
            "29: Predicted: confusion, Actual: caring\n",
            "\n",
            "Model Output: confusion\n",
            "Text: I have a 1080ti and an i7 8700k and on very fast at 40k bitrate I'm already hitting 50% cpu usage on obs alone, any suggestions?\n",
            "30: Predicted: confusion, Actual: curiosity\n",
            "\n",
            "Model Output: gratitude\n",
            "Text: Ask and you shall receive. Thanks man\n",
            "31: Predicted: gratitude, Actual: neutral\n",
            "\n",
            "Model Output: confusion\n",
            "Text: I can't tell if its a guy or a girl and i don't know if I love or hate that\n",
            "32: Predicted: confusion, Actual: confusion\n",
            "\n",
            "Model Output: relief\n",
            "Text: That's even cheaper than I had figured.. there's still the \"to you\" bit... OP needs to post an Amazon wishlist.\n",
            "33: Predicted: relief, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: How ignorant do you have to be to not understand that [NAME] IS NOT ALT-RIGHT. HE PREACHES COMMON SENSE THAT NO ONE APPARENTLY ADHERES TO CLEARLY.\n",
            "34: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: annoyance\n",
            "Text: Man you sure are obsessed with race. Much like [NAME].\n",
            "35: Predicted: annoyance, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: I believe that show scared straight was actually a massive failure, it got viewers but concept of it never resulted in the kids actually being 'scared straight'.\n",
            "36: Predicted: anger, Actual: fear\n",
            "\n",
            "Model Output: anger\n",
            "Text: [NAME] also played terrible. It‚Äôs only the past couple of games has it started to click.\n",
            "37: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: I like how she calls him racist, when he didn‚Äôt actually say anything racist.\n",
            "38: Predicted: anger, Actual: admiration\n",
            "\n",
            "Model Output: surprise\n",
            "Text: Can‚Äôt imagine taking on that contract\n",
            "39: Predicted: surprise, Actual: disapproval\n",
            "\n",
            "Model Output: anger\n",
            "Text: WHY IS THERE WOMAN ON MY ENTERTAINMENT??????????\n",
            "40: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: It was worth mentioning anyways, the Packers are a special kind of awful.\n",
            "41: Predicted: anger, Actual: annoyance\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Please post any news you may find about her fate. This is so sad and so wrong. We need to know what happens to her.\n",
            "42: Predicted: sadness, Actual: gratitude\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: My father needs to read this! ...I may be wrong tho\n",
            "43: Predicted: curiosity, Actual: surprise\n",
            "\n",
            "Model Output: pain\n",
            "Text: That last sentence pains me\n",
            "44: Predicted: invalid, Actual: sadness\n",
            "\n",
            "Model Output: confusion\n",
            "Text: I mean, it certainly is weird but don't kinkshame\n",
            "45: Predicted: confusion, Actual: disgust\n",
            "\n",
            "Model Output: gratitude\n",
            "Text: Thank you, this is genuinely helpful. \n",
            "46: Predicted: gratitude, Actual: gratitude\n",
            "\n",
            "Model Output: anger\n",
            "Text: If he would have landed that swing, it would have decapitated that guy.\n",
            "47: Predicted: anger, Actual: disappointment\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Also I know a guy in a metal band called Artificial Sacrifice. They're really small and always looking for gigs. Check them out on Instagram\n",
            "48: Predicted: curiosity, Actual: neutral\n",
            "\n",
            "Model Output: pride\n",
            "Text: I can't remember a time during my early childhood where I didn't dress up like my mom and almost break my little ankles in her heels\n",
            "49: Predicted: pride, Actual: sadness\n",
            "\n",
            "Model Output: embarrassment\n",
            "Text: When he puts it in too fast and you don't have time to let your walls expands, this is exactly how it feels... \n",
            "50: Predicted: embarrassment, Actual: sadness\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Spoiler >!Loli Girl about to grow up in 2 episodes!<\n",
            "51: Predicted: curiosity, Actual: realization\n",
            "\n",
            "Model Output: anger\n",
            "Text: Ok shady flags are flapping so hard I was knocked out of my chair.\n",
            "52: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: amusement\n",
            "Text: Love it when the wife goes lets it go retro. Also love it when she gets that puppy waxed.\n",
            "53: Predicted: amusement, Actual: love\n",
            "\n",
            "Model Output: pity\n",
            "Text: Poor eyes. Can‚Äôt even see and is sent to the trunk.\n",
            "54: Predicted: invalid, Actual: caring\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Have to run some plays for him, he is guarded pretty tightly. \n",
            "55: Predicted: curiosity, Actual: neutral\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Probably. Wouldn't be surprised if our current generation essentially lives forever...or well until something like climate change puts a stop to it.\n",
            "56: Predicted: curiosity, Actual: surprise\n",
            "\n",
            "Model Output: anger\n",
            "Text: And feminism, the movement that started with fighting for the right for women to vote hates elections. \n",
            "57: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: relief\n",
            "Text: Exactly. It was The fact that everything happened and ended so suddenly that left me speechless.\n",
            "58: Predicted: relief, Actual: disappointment\n",
            "\n",
            "Model Output: relief\n",
            "Text: You're just mad because you're starting to realize that you're reaaaaaaaaaaally close to being a [NAME].\n",
            "59: Predicted: relief, Actual: anger\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Bedside Rounds (which that user puts out) is also a very interesting podcast on medical history.\n",
            "60: Predicted: curiosity, Actual: approval\n",
            "\n",
            "Model Output: love\n",
            "Text: I love this. I really want to hug [NAME] now.\n",
            "61: Predicted: love, Actual: optimism\n",
            "\n",
            "Model Output: anger\n",
            "Text: This is the type of response I give people that are toxic in Rocket League. They're just fake and cowards lol\n",
            "62: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: disgust\n",
            "Text: She also takes it off to shower, work out, wash dishes, etc. \n",
            "63: Predicted: disgust, Actual: disgust\n",
            "\n",
            "Model Output: relief\n",
            "Text: Of those affiliated with TFSA? Not heard about that. I do recall there being a general amnesty to those involved in looting though...\n",
            "64: Predicted: relief, Actual: confusion\n",
            "\n",
            "Model Output: pride\n",
            "Text: [NAME] and [NAME] are my favorite gay couple\n",
            "65: Predicted: pride, Actual: love\n",
            "\n",
            "Model Output: anger\n",
            "Text: That dude is Russian intelligence\n",
            "66: Predicted: anger, Actual: admiration\n",
            "\n",
            "Model Output: anger\n",
            "Text: Here's my guess: they are educated, and tired of being treated like second class citizens by the D party for being men.\n",
            "67: Predicted: anger, Actual: confusion\n",
            "\n",
            "Model Output: admiration\n",
            "Text: The perfectly timed dessert, and delicious looking, too!\n",
            "68: Predicted: admiration, Actual: admiration\n",
            "\n",
            "Model Output: love\n",
            "Text: I love [NAME]. I really love him. Xx\n",
            "69: Predicted: love, Actual: love\n",
            "\n",
            "Model Output: anger\n",
            "Text: My bf uses profanity, and I don't care. There's no reason for misogyny though.\n",
            "70: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: This level of naivety is off the charts\n",
            "71: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: annoyance\n",
            "Text: That's what happens when you're high.\n",
            "72: Predicted: annoyance, Actual: annoyance\n",
            "\n",
            "Model Output: anger\n",
            "Text: Kill me, [NAME] \n",
            "73: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: excitement\n",
            "Text: That‚Äôs awesome!\n",
            "74: Predicted: excitement, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: If only life was a garbage isekai anime. Too bad the genre is just pandering and blatant self-insert/wish fulfillment. Oh well.\n",
            "75: Predicted: anger, Actual: annoyance\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: I never thought to put protein into my oatmeal. I may try that tomorrow.\n",
            "76: Predicted: curiosity, Actual: neutral\n",
            "\n",
            "Model Output: amusement\n",
            "Text: [NAME] and [NAME] at crunch time. I must be dreaming!\n",
            "77: Predicted: amusement, Actual: amusement\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Damn, and now this GIF has become bittersweet. I really wanted him to become a revolutionary icon.\n",
            "78: Predicted: sadness, Actual: desire\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Miss you...\n",
            "79: Predicted: sadness, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: Honestly this type of support sounds very shallow. It‚Äôs not like you‚Äôre gonna support women in any way when their babies are born.\n",
            "80: Predicted: anger, Actual: disapproval\n",
            "\n",
            "Model Output: curiosity\n",
            "Text: Sorry to link it here but would love to get some opinions on this post\n",
            "81: Predicted: curiosity, Actual: love\n",
            "\n",
            "Model Output: surprise\n",
            "Text: Refreshingly low upvote/downvote ratio.\n",
            "82: Predicted: surprise, Actual: approval\n",
            "\n",
            "Model Output: concern\n",
            "Text: Is [NAME] okay?\n",
            "83: Predicted: invalid, Actual: caring\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# PRETRAINED_PROMPT = \"Classify the emotion of this text. Text: {input} Emotion:\"\n",
        "\n",
        "PRETRAINED_PROMPT = (\n",
        "    \"Sentence: \\\"I've never felt worse.\\\"\\n\"\n",
        "    \"Allowed emotions: admiration,amusement,anger,annoyance,approval,caring,confusion,curiosity,desire,disappointment,disapproval,disgust,embarrassment,excitement,fear,gratitude,grief,joy,love,nervousness,optimism,pride,realization,relief,remorse,sadness,surprise,neutral\\n\"\n",
        "    \"Emotion = sadness\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"This is the best day ever!\\\"\\n\"\n",
        "    \"Allowed emotions: admiration,amusement,anger,annoyance,approval,caring,confusion,curiosity,desire,disappointment,disapproval,disgust,embarrassment,excitement,fear,gratitude,grief,joy,love,nervousness,optimism,pride,realization,relief,remorse,sadness,surprise,neutral\\n\"\n",
        "    \"Emotion = joy\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"Wow! I can't believe it!\\\"\\n\"\n",
        "    \"Allowed emotions: admiration,amusement,anger,annoyance,approval,caring,confusion,curiosity,desire,disappointment,disapproval,disgust,embarrassment,excitement,fear,gratitude,grief,joy,love,nervousness,optimism,pride,realization,relief,remorse,sadness,surprise,neutral\\n\"\n",
        "    \"Emotion = surprise\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"That's so rude!\\\"\\n\"\n",
        "    \"Allowed emotions: admiration,amusement,anger,annoyance,approval,caring,confusion,curiosity,desire,disappointment,disapproval,disgust,embarrassment,excitement,fear,gratitude,grief,joy,love,nervousness,optimism,pride,realization,relief,remorse,sadness,surprise,neutral\\n\"\n",
        "    \"Emotion = anger\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"{input}\\\"\\n\"\n",
        "    \"Allowed emotions: admiration,amusement,anger,annoyance,approval,caring,confusion,curiosity,desire,disappointment,disapproval,disgust,embarrassment,excitement,fear,gratitude,grief,joy,love,nervousness,optimism,pride,realization,relief,remorse,sadness,surprise,neutral\\n\"\n",
        "    \"Emotion =\"\n",
        ")\n",
        "\n",
        "\n",
        "EMOTIONS = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
        "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
        "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
        "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
        "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "\n",
        "def get_emotion(row, emotions=EMOTIONS):\n",
        "    active = [e for e in emotions if row[e] == 1]\n",
        "    return active[0] if len(active) == 1 else None\n",
        "\n",
        "# Function to map model output to closest emotion label\n",
        "def map_to_emotion_label(model_output, emotions=EMOTIONS):\n",
        "    model_output = model_output.lower().strip()\n",
        "\n",
        "    # Exact match first\n",
        "    if model_output in emotions:\n",
        "        return model_output\n",
        "\n",
        "    # Token-based fallback\n",
        "    for emotion in emotions:\n",
        "        if emotion in model_output.split():\n",
        "            return emotion\n",
        "\n",
        "    return \"invalid\"\n",
        "\n",
        "\n",
        "df_all[\"emotion\"] = df_all.apply(get_emotion, axis=1)\n",
        "\n",
        "# Keep only valid, non-neutral labels\n",
        "df_eval = df_all[\n",
        "    df_all[\"emotion\"].isin(EMOTIONS)\n",
        "].copy()\n",
        "\n",
        "# Sample a subset for testing (adjust sample_size as needed)\n",
        "sample_size = min(500, len(df_all))  # Change to len(df_all) to test all\n",
        "\n",
        "df_sample = df_eval.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Testing emotion classification on {len(df_sample)} samples...\\n\")\n",
        "\n",
        "predictions = []\n",
        "actual_labels = []\n",
        "correct = 0\n",
        "invalid = 0\n",
        "\n",
        "for idx, (_, row) in enumerate(df_sample.iterrows()):\n",
        "    text = row[\"text\"]\n",
        "    actual_emotion = row[\"emotion\"]\n",
        "\n",
        "    # Generate prediction\n",
        "    model_output = run_model(PRETRAINED_PROMPT.replace(\"{input}\", text),\n",
        "                            model_pretrained, tokenizer_pretrained)\n",
        "    print(f\"Model Output: {model_output}\")\n",
        "    predicted_emotion = map_to_emotion_label(model_output)\n",
        "\n",
        "    predictions.append(predicted_emotion)\n",
        "    actual_labels.append(actual_emotion)\n",
        "\n",
        "    if predicted_emotion == \"invalid\":\n",
        "        invalid += 1\n",
        "\n",
        "    elif predicted_emotion == actual_emotion:\n",
        "        correct += 1\n",
        "\n",
        "    # # Print first 10 examples\n",
        "    # if idx < 10:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"{idx}: Predicted: {predicted_emotion}, Actual: {actual_emotion}\\n\")\n",
        "\n",
        "# Calculate accuracy and create results dataframe\n",
        "accuracy = (correct / len(df_sample)) * 100\n",
        "print(f\"Accuracy: {correct}/{len(df_sample)} = {accuracy:.2f}%\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = df_sample.copy()\n",
        "results_df['predicted_emotion'] = predictions\n",
        "results_df['correct'] = [p == a for p, a in zip(predictions, actual_labels)]\n",
        "\n",
        "print(f\"\\nCorrect predictions by emotion:\")\n",
        "print(results_df.groupby('emotion')['correct'].agg(['sum', 'count', 'mean']))\n",
        "\n",
        "print(f\"\\nPredicted vs Actual Emotion Distribution:\")\n",
        "print(pd.DataFrame({\n",
        "    'Predicted': predictions,\n",
        "    'Actual': actual_labels\n",
        "}).value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DSn8RPPBi4fy",
      "metadata": {
        "id": "DSn8RPPBi4fy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(\n",
        "    actual_labels,\n",
        "    predictions,\n",
        "    labels=[\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
        "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
        "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
        "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
        "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eBBNkDNJi92Q",
      "metadata": {
        "id": "eBBNkDNJi92Q"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(\n",
        "    pd.Series(actual_labels, name=\"Actual\"),\n",
        "    pd.Series(predictions, name=\"Predicted\"),\n",
        "    normalize=\"index\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "CqYqI1MfyCOi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CqYqI1MfyCOi",
        "outputId": "6818966e-4e93-45ce-e3f7-cdb4ab15027e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing emotion classification on 500 samples...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Output: sadness\n",
            "Text: You are the parent your grandson deserves. Your daughter will regret the pain she has caused.\n",
            "0: Predicted: sadness, Actual: sadness\n",
            "\n",
            "Model Output: joy\n",
            "Text: Congrats for coming out üòÅüòÅ\n",
            "1: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: >the satisfaction of knowing how bent out of shape you're getting What a sad existence.\n",
            "2: Predicted: sadness, Actual: sadness\n",
            "\n",
            "Model Output: fear\n",
            "Text: So..the head [NAME], is afraid of being rammed?\n",
            "3: Predicted: fear, Actual: neutral\n",
            "\n",
            "Model Output: disgust\n",
            "Text: Her upper lip always looks terrible - such an easy fix, can‚Äôt believe she is so vain and never bothers to wax \n",
            "4: Predicted: disgust, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: Get your mom professional help. Any grow person who runs away from their own home afterargument definitely has mental health issues\n",
            "5: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: Don't confused sex with gender. Gender identity can't be boiled down to beards and balls.\n",
            "6: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: None (This sentence is likely a joke or a statement of opinion)\n",
            "Text: The real NZ L&p is lungies & puha!\n",
            "7: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: joy\n",
            "Text: Pensacola Florida 2018 not enough words to describe it. Simply amazing.\n",
            "8: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: I‚Äôm anything but little you little puppy kicker\n",
            "9: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: ITT: Hipsters. ‚ÄúI liked this band before they got all popular and shit.‚Äù\n",
            "10: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: None\n",
            "Text: Most people buy them new because the risk of them being involved in some sort of accident is just too high.\n",
            "11: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: Better yet, get rid of them if they are non-essential. Why are my tax dollars paying for worthless jobs?\n",
            "12: Predicted: anger, Actual: surprise\n",
            "\n",
            "Model Output: sadness\n",
            "Text: The same here. It's hard to be alone, but not as hard as the eggshells and abuse.\n",
            "13: Predicted: sadness, Actual: sadness\n",
            "\n",
            "Model Output: None\n",
            "Text: we need his euro step comp with this in the background [NAME]\n",
            "14: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: disgust\n",
            "Text: Ate too much ‚Äúhot chips‚Äù\n",
            "15: Predicted: disgust, Actual: joy\n",
            "\n",
            "Model Output: None (This sentence is nonsensical and doesn't convey any emotion)\n",
            "Text: Yo your eyes are glued wide open like aderal and you move your head all the way back you are done\n",
            "16: Predicted: invalid, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Everyone was fine so I guess it‚Äôs okay\n",
            "17: Predicted: sadness, Actual: joy\n",
            "\n",
            "Model Output: none\n",
            "Text: Hope you had a great time!\n",
            "18: Predicted: invalid, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: The concern is that there is a very high likelihood of his health deteriorating in one way or another before he can complete two full terms.\n",
            "19: Predicted: sadness, Actual: neutral\n",
            "\n",
            "Model Output: None\n",
            "Text: the socially weak kid who is also: - neurotypical - not too short - not too ugly so basically a normie\n",
            "20: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: surprise\n",
            "Text: I like [NAME]. Sue me.\n",
            "21: Predicted: surprise, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: Just next her. She'll never find a SD anyway, she's all talk.\n",
            "22: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: None (This is a greeting)\n",
            "Text: Hail [NAME]!\n",
            "23: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: None (This sentence is grammatically incorrect)\n",
            "Text: Yea ur probably be super drowsy tho.\n",
            "24: Predicted: invalid, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Not what I wanted to read today. :(\n",
            "25: Predicted: sadness, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: Speaking of not having a brain, that seems pretty ironic coming from someone with your attitude.\n",
            "26: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: joy\n",
            "Text: Okay I can see why that is one glorious beard\n",
            "27: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: Are you joking? 538 has him hovering around -15 net approval. He won‚Äôt win unless Dems nominate someone like [NAME] again at this rate.\n",
            "28: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: anger\n",
            "Text: 100%. Please ignore him. He's annoying af and survives off attention\n",
            "29: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: joy\n",
            "Text: Haha yep. Thanks. Fixed it!\n",
            "30: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Shaving her legs, hoping the video didn‚Äôt cut short and that‚Äôs all she shaved in there....\n",
            "31: Predicted: sadness, Actual: joy\n",
            "\n",
            "Model Output: joy\n",
            "Text: I kinda love it. Not sure what the theme of the party would be but... its cool.\n",
            "32: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: joy\n",
            "Text: He drives to the basket like [NAME] used to. And can shoot the 3. And he's super clutch. Love the dude\n",
            "33: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: We are killing this planet\n",
            "34: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: joy\n",
            "Text: First and foremost you have to Love YOURSELF!!\n",
            "35: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: None\n",
            "Text: I ride our half-orc barbarian by clinging to his back. He hasn't seemed to notice yet.\n",
            "36: Predicted: invalid, Actual: fear\n",
            "\n",
            "Model Output: fear\n",
            "Text: I feel like the cameraman is having a seizure or something\n",
            "37: Predicted: fear, Actual: neutral\n",
            "\n",
            "Model Output: joy\n",
            "Text: This is brilliant!\n",
            "38: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: And spending extravagant amounts on campaigning to impress the poor fools with the money that could have been used to improve their lives.\n",
            "39: Predicted: anger, Actual: disgust\n",
            "\n",
            "Model Output: None\n",
            "Text: He's hanging with friends...TV on, volume down. People asking, how are things in Detroit and he just shrugs and shakes his head\n",
            "40: Predicted: invalid, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: Stop attacking me\n",
            "41: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: None\n",
            "Text: You can see [NAME] weapon on the plane in the background when [NAME] is talking\n",
            "42: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: joy\n",
            "Text: Aha! Thank you for that.\n",
            "43: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: [NAME] isn't facilitating anything between South and North Korea. The only thing he can take credit for is no longer threatening \"fire and fury.\"\n",
            "44: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: sadness\n",
            "Text: I think this does help less good pupils to learn it better and agree with the teachers. In my class thats the biggest problem when it comes to french\n",
            "45: Predicted: sadness, Actual: joy\n",
            "\n",
            "Model Output: None (This is a joke, and the emotion is not relevant)\n",
            "Text: Thank you - you may call me daddy.\n",
            "46: Predicted: invalid, Actual: joy\n",
            "\n",
            "Model Output: sadness\n",
            "Text: oh no I agree that its edited, but I think she has other things going on too.. BDD at least \n",
            "47: Predicted: sadness, Actual: joy\n",
            "\n",
            "Model Output: None\n",
            "Text: Pretend it's a multiplication table. Memorize a bit at a time.\n",
            "48: Predicted: invalid, Actual: neutral\n",
            "\n",
            "Model Output: joy\n",
            "Text: THANK YOU!\n",
            "49: Predicted: joy, Actual: joy\n",
            "\n",
            "Model Output: anger\n",
            "Text: ARGH!!\n",
            "50: Predicted: anger, Actual: anger\n",
            "\n",
            "Model Output: anger\n",
            "Text: Print outs?!?!?! You tree killing monster!!!!\n",
            "51: Predicted: anger, Actual: neutral\n",
            "\n",
            "Model Output: sadness\n",
            "Text: Might not have fallen if little miss [NAME] didn‚Äôt have to pull her skirt back.\n",
            "52: Predicted: sadness, Actual: surprise\n",
            "\n",
            "Model Output: anger\n",
            "Text: Typical left, calling people [NAME] just so they can hate them\n",
            "53: Predicted: anger, Actual: fear\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-420502917.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Generate prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     model_output = run_model(PRETRAINED_PROMPT.replace(\"{input}\", text),\n\u001b[0m\u001b[1;32m     76\u001b[0m                             model_instruction_tuned, tokenizer_instruction_tuned)\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model Output: {model_output}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2204265749.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(prompt, model, tokenizer, apply_chat_template, generation_kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   output_ids = model.generate(\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mgeneration_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m         )\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         )\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma2/modeling_gemma2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# See https://github.com/huggingface/transformers/pull/29402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# PRETRAINED_PROMPT = \"Classify the emotion of this text. Text: {input} Emotion:\"\n",
        "\n",
        "PRETRAINED_PROMPT = (\n",
        "    \"Sentence: \\\"I've never felt worse.\\\"\\n\"\n",
        "    \"Allowed emotions: joy, surprise, anger, sadness, disgust, fear\\n\"\n",
        "    \"Emotion = sadness\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"This is the best day ever!\\\"\\n\"\n",
        "    \"Allowed emotions: joy, surprise, anger, sadness, disgust, fear\\n\"\n",
        "    \"Emotion = joy\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"Wow! I can't believe it!\\\"\\n\"\n",
        "    \"Allowed emotions: joy, surprise, anger, sadness, disgust, fear\\n\"\n",
        "    \"Emotion = surprise\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"That's so rude!\\\"\\n\"\n",
        "    \"Allowed emotions: joy, surprise, anger, sadness, disgust, fear\\n\"\n",
        "    \"Emotion = anger\\n\\n\"\n",
        "\n",
        "    \"Sentence: \\\"{input}\\\"\\n\"\n",
        "    \"Allowed emotions: joy, surprise, anger, sadness, disgust, fear\\n\"\n",
        "    \"Emotion =\"\n",
        ")\n",
        "\n",
        "\n",
        "EMOTIONS = [\n",
        "    \"anger\", \"disgust\", \"fear\",\n",
        "    \"joy\", \"sadness\", \"surprise\", \"neutral\"\n",
        "]\n",
        "\n",
        "def get_emotion(row, emotions=EMOTIONS):\n",
        "    active = [e for e in emotions if row[e] == 1]\n",
        "    return active[0] if len(active) == 1 else None\n",
        "\n",
        "# Function to map model output to closest emotion label\n",
        "def map_to_emotion_label(model_output, emotions=EMOTIONS):\n",
        "    model_output = model_output.lower().strip()\n",
        "\n",
        "    # Exact match first\n",
        "    if model_output in emotions:\n",
        "        return model_output\n",
        "\n",
        "    # Token-based fallback\n",
        "    for emotion in emotions:\n",
        "        if emotion in model_output.split():\n",
        "            return emotion\n",
        "\n",
        "    return \"invalid\"\n",
        "\n",
        "\n",
        "df_all[\"emotion\"] = df_all.apply(get_emotion, axis=1)\n",
        "\n",
        "# Keep only valid, non-neutral labels\n",
        "df_eval = df_all[\n",
        "    df_all[\"emotion\"].isin(EMOTIONS)\n",
        "].copy()\n",
        "\n",
        "# Sample a subset for testing (adjust sample_size as needed)\n",
        "sample_size = min(500, len(df_all))  # Change to len(df_all) to test all\n",
        "\n",
        "df_sample = df_eval.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Testing emotion classification on {len(df_sample)} samples...\\n\")\n",
        "\n",
        "pretrained_predictions = []\n",
        "pretrained_actual_labels = []\n",
        "correct = 0\n",
        "invalid = 0\n",
        "\n",
        "for idx, (_, row) in enumerate(df_sample.iterrows()):\n",
        "    text = row[\"text\"]\n",
        "    actual_emotion = row[\"emotion\"]\n",
        "\n",
        "    # Generate prediction\n",
        "    model_output = run_model(PRETRAINED_PROMPT.replace(\"{input}\", text),\n",
        "                            model_instruction_tuned, tokenizer_instruction_tuned)\n",
        "    print(f\"Model Output: {model_output}\")\n",
        "    predicted_emotion = map_to_emotion_label(model_output)\n",
        "\n",
        "    pretrained_predictions.append(predicted_emotion)\n",
        "    pretrained_actual_labels.append(actual_emotion)\n",
        "\n",
        "    if predicted_emotion == \"invalid\":\n",
        "        invalid += 1\n",
        "\n",
        "    elif predicted_emotion == actual_emotion:\n",
        "        correct += 1\n",
        "\n",
        "    # # Print first 10 examples\n",
        "    # if idx < 10:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"{idx}: Predicted: {predicted_emotion}, Actual: {actual_emotion}\\n\")\n",
        "\n",
        "# Calculate accuracy and create results dataframe\n",
        "accuracy = (correct / len(df_sample)) * 100\n",
        "print(f\"Accuracy: {correct}/{len(df_sample)} = {accuracy:.2f}%\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = df_sample.copy()\n",
        "results_df['predicted_emotion'] = pretrained_predictions\n",
        "results_df['correct'] = [p == a for p, a in zip(pretrained_predictions, pretrained_actual_labels)]\n",
        "\n",
        "print(f\"\\nCorrect predictions by emotion:\")\n",
        "print(results_df.groupby('emotion')['correct'].agg(['sum', 'count', 'mean']))\n",
        "\n",
        "print(f\"\\nPredicted vs Actual Emotion Distribution:\")\n",
        "print(pd.DataFrame({\n",
        "    'Predicted': pretrained_predictions,\n",
        "    'Actual': pretrained_actual_labels\n",
        "}).value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8MBpZsLyjbB",
      "metadata": {
        "id": "e8MBpZsLyjbB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(\n",
        "    pretrained_actual_labels,\n",
        "    pretrained_predictions,\n",
        "    labels=[\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"neutral\"]\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dbf214de76844fb98cefb64ab9f2fc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc1dc62bfcb4ad2b1ecf140098d02cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c300774212f844d78aca57f40c9bc4b2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_248575939d6040dea530de77d88c3dc8",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "1350df1a74c4425799d5c52adc0e0df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1f4bde4b844b43b51f7e348fa8c477",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6b4c202d9f9049c1b22791ff4c136aad",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "248575939d6040dea530de77d88c3dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "259da2afa42946728645391aa6d714c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42bb081167504bedac17e611477f4945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dda80dd9d714df188ff3d50e26f7642": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fc1dc62bfcb4ad2b1ecf140098d02cc",
              "IPY_MODEL_cc60bcb5a3a0454cbd227873402701a0",
              "IPY_MODEL_57293a7cdd56452eb5b27c92fb524604"
            ],
            "layout": "IPY_MODEL_fe9e272bf51a454bb04b360460b0db07"
          }
        },
        "57293a7cdd56452eb5b27c92fb524604": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42bb081167504bedac17e611477f4945",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9e74dbd4ec8d4ae4975c77f5c2f4ec9e",
            "value": "‚Äá2/2‚Äá[00:21&lt;00:00,‚Äá‚Äá9.10s/it]"
          }
        },
        "5e563994ff7d43c6b836d371086e2f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b4c202d9f9049c1b22791ff4c136aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "879cc177d2f64f25bac81de6efa88266": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd4b26addd5349e6895b0ad80fce18ce",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ba8b928255ee421291f6493246087e8c",
            "value": "‚Äá3/3‚Äá[00:58&lt;00:00,‚Äá16.07s/it]"
          }
        },
        "8f1f4bde4b844b43b51f7e348fa8c477": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e74dbd4ec8d4ae4975c77f5c2f4ec9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba8b928255ee421291f6493246087e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c03aca2eae0d4d76beb69edaac35b53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1350df1a74c4425799d5c52adc0e0df2",
              "IPY_MODEL_cb537eca95e54e1b848c0250504a5fc7",
              "IPY_MODEL_879cc177d2f64f25bac81de6efa88266"
            ],
            "layout": "IPY_MODEL_f5c0c20c15864f8a9069ef940f2617a1"
          }
        },
        "c300774212f844d78aca57f40c9bc4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb537eca95e54e1b848c0250504a5fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dbf214de76844fb98cefb64ab9f2fc7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e563994ff7d43c6b836d371086e2f78",
            "value": 3
          }
        },
        "cc60bcb5a3a0454cbd227873402701a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259da2afa42946728645391aa6d714c7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1bf2697659c438f9e1cc8818bd287dd",
            "value": 2
          }
        },
        "dd4b26addd5349e6895b0ad80fce18ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1bf2697659c438f9e1cc8818bd287dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5c0c20c15864f8a9069ef940f2617a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9e272bf51a454bb04b360460b0db07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
