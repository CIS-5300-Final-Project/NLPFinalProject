{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e196312",
   "metadata": {},
   "source": [
    "# ModernBERT Emotion Classification\n",
    "This notebook trains a ModernBERT model on the GoEmotions dataset and then fine-tunes it on the presidential speeches dataset.\n",
    "ModernBERT is a modernized version of BERT with architectural improvements (Rotary Embeddings, Unpadding, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c93eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.48.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (1.8.0)\n",
      "Requirement already satisfied: protobuf in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (6.33.2)\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-eywhk3qn/overlay/lib/python3.14/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-eywhk3qn/overlay/lib/python3.14/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-eywhk3qn/overlay/lib/python3.14/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-eywhk3qn/overlay/lib/python3.14/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m22\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'torch'\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31mERROR: Failed to build 'flash-attn' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# ModernBERT requires a recent version of transformers\n",
    "%pip install \"transformers>=4.48.0\" datasets accelerate scikit-learn protobuf flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e207759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/miniconda3/envs/modernbert_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d446b8",
   "metadata": {},
   "source": [
    "### Load Data and Define Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20986823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 43410, Val: 5426, Test: 5427\n"
     ]
    }
   ],
   "source": [
    "# Load the GoEmotions dataset\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_LABELS = len(LABELS)\n",
    "\n",
    "def convert_to_df(split):\n",
    "    data = dataset[split]\n",
    "    rows = []\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]['text']\n",
    "        label_ids = data[i]['labels']\n",
    "        label_vec = [1 if j in label_ids else 0 for j in range(NUM_LABELS)]\n",
    "        rows.append([text] + label_vec)\n",
    "    return pd.DataFrame(rows, columns=['text'] + LABELS)\n",
    "\n",
    "train_df = convert_to_df('train')\n",
    "val_df = convert_to_df('validation')\n",
    "test_df = convert_to_df('test')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520a1ec",
   "metadata": {},
   "source": [
    "### Initialize ModernBERT Tokenizer and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8f8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ModernBERT base tokenizer\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512): # ModernBERT supports up to 8192, but we'll stick to 512 for speed\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = LABELS\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.loc[idx, 'text'])\n",
    "        labels = self.data.loc[idx, self.labels].values.astype(float)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = EmotionDataset(train_df, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a60ce6",
   "metadata": {},
   "source": [
    "### Initialize ModernBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b204900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answerdotai/ModernBERT-base loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 5e-5 # ModernBERT can often handle slightly higher LRs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Model {MODEL_NAME} loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f6bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            preds = (probs > threshold).float()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3331901",
   "metadata": {},
   "source": [
    "### Train ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92556511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2714 [00:00<?, ?it/s]/home/xiang/miniconda3/envs/modernbert_env/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 2714/2714 [18:43<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:55<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0829\n",
      "Validation F1 (micro): 0.5737\n",
      "Validation F1 (macro): 0.4184\n",
      "Saved best model!\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [17:10<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:44<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0812\n",
      "Validation F1 (micro): 0.5753\n",
      "Validation F1 (macro): 0.4555\n",
      "Saved best model!\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [16:15<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:40<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0932\n",
      "Validation F1 (micro): 0.5909\n",
      "Validation F1 (macro): 0.4920\n",
      "Saved best model!\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [16:02<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:40<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1136\n",
      "Validation F1 (micro): 0.5811\n",
      "Validation F1 (macro): 0.4903\n",
      "\n",
      "Best Validation F1 (macro): 0.4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_results = evaluate(model, val_loader, device)\n",
    "    print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "    print(f\"Validation F1 (micro): {val_results['f1_micro']:.4f}\")\n",
    "    print(f\"Validation F1 (macro): {val_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    if val_results['f1_macro'] > best_f1:\n",
    "        best_f1 = val_results['f1_macro']\n",
    "        torch.save(model.state_dict(), 'best_modernbert_model.pt')\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "print(f\"\\nBest Validation F1 (macro): {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df7ae9",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a820022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:41<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MODERNBERT TEST RESULTS\n",
      "==================================================\n",
      "Test F1 (micro): 0.5928\n",
      "Test F1 (macro): 0.4940\n",
      "\n",
      "Detailed Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.70      0.70      0.70       504\n",
      "     amusement       0.78      0.82      0.80       264\n",
      "         anger       0.56      0.41      0.48       198\n",
      "     annoyance       0.43      0.28      0.34       320\n",
      "      approval       0.57      0.27      0.37       351\n",
      "        caring       0.47      0.30      0.36       135\n",
      "     confusion       0.59      0.33      0.42       153\n",
      "     curiosity       0.52      0.58      0.55       284\n",
      "        desire       0.57      0.47      0.51        83\n",
      "disappointment       0.38      0.12      0.18       151\n",
      "   disapproval       0.48      0.31      0.37       267\n",
      "       disgust       0.62      0.42      0.50       123\n",
      " embarrassment       0.63      0.32      0.43        37\n",
      "    excitement       0.55      0.34      0.42       103\n",
      "          fear       0.72      0.62      0.66        78\n",
      "     gratitude       0.93      0.90      0.91       352\n",
      "         grief       0.67      0.33      0.44         6\n",
      "           joy       0.69      0.55      0.61       161\n",
      "          love       0.78      0.83      0.80       238\n",
      "   nervousness       0.47      0.35      0.40        23\n",
      "      optimism       0.69      0.42      0.53       186\n",
      "         pride       0.67      0.12      0.21        16\n",
      "   realization       0.45      0.17      0.25       145\n",
      "        relief       0.50      0.09      0.15        11\n",
      "       remorse       0.57      0.79      0.66        56\n",
      "       sadness       0.66      0.53      0.59       156\n",
      "      surprise       0.65      0.44      0.52       141\n",
      "       neutral       0.67      0.64      0.66      1787\n",
      "\n",
      "     micro avg       0.65      0.54      0.59      6329\n",
      "     macro avg       0.61      0.44      0.49      6329\n",
      "  weighted avg       0.64      0.54      0.58      6329\n",
      "   samples avg       0.58      0.57      0.56      6329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_modernbert_model.pt'))\n",
    "test_results = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODERNBERT TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test F1 (micro): {test_results['f1_micro']:.4f}\")\n",
    "print(f\"Test F1 (macro): {test_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    test_results['labels'], \n",
    "    test_results['predictions'], \n",
    "    target_names=LABELS,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995a236",
   "metadata": {},
   "source": [
    "## Domain Adaptation: Presidential Speeches\n",
    "Fine-tune the best ModernBERT model on the presidential speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc17c77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presidential Dataset Shape: (995, 67)\n",
      "Using text column: speech\n"
     ]
    }
   ],
   "source": [
    "# Load Presidential Data\n",
    "pres_df = pd.read_csv(\"data/presidential_speeches_goemotions_labeled.csv\")\n",
    "print(f\"Presidential Dataset Shape: {pres_df.shape}\")\n",
    "\n",
    "# Identify text column\n",
    "text_col = None\n",
    "for col in ['speech', 'Speech', 'transcript', 'Transcript', 'text', 'Text', 'content']:\n",
    "    if col in pres_df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError(f\"Could not find text column. Available columns: {pres_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using text column: {text_col}\")\n",
    "\n",
    "# Train-test split\n",
    "pres_train_df, pres_test_df = train_test_split(pres_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Dataset class for Presidential data\n",
    "class PresidentialDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, text_col, max_length=512):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_col = text_col\n",
    "        self.labels = LABELS\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.loc[idx, self.text_col])\n",
    "        if pd.isna(text):\n",
    "            text = \"\"\n",
    "        labels = self.data.loc[idx, self.labels].values.astype(float)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "pres_train_dataset = PresidentialDataset(pres_train_df, tokenizer, text_col)\n",
    "pres_test_dataset = PresidentialDataset(pres_test_df, tokenizer, text_col)\n",
    "\n",
    "pres_train_loader = DataLoader(pres_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "pres_test_loader = DataLoader(pres_test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0aec216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning ModernBERT on presidential data for 3 epochs...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:17<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.80it/s]\n",
      "/home/xiang/miniconda3/envs/modernbert_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0494\n",
      "Test F1 (micro): 0.7120\n",
      "Test F1 (macro): 0.0522\n",
      "Saved best presidential model!\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:18<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.77it/s]\n",
      "/home/xiang/miniconda3/envs/modernbert_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0496\n",
      "Test F1 (micro): 0.7254\n",
      "Test F1 (macro): 0.0503\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:17<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.31it/s]\n",
      "/home/xiang/miniconda3/envs/modernbert_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0488\n",
      "Test F1 (micro): 0.7202\n",
      "Test F1 (macro): 0.0526\n",
      "Saved best presidential model!\n",
      "\n",
      "Best Presidential Test F1 (macro): 0.0526\n"
     ]
    }
   ],
   "source": [
    "# Load the best ModernBERT model\n",
    "model.load_state_dict(torch.load('best_modernbert_model.pt'))\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "FINE_TUNE_LR = 1e-5\n",
    "FINE_TUNE_EPOCHS = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=FINE_TUNE_LR, weight_decay=0.01)\n",
    "total_steps = len(pres_train_loader) * FINE_TUNE_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "print(f\"Fine-tuning ModernBERT on presidential data for {FINE_TUNE_EPOCHS} epochs...\")\n",
    "\n",
    "best_pres_f1 = 0\n",
    "\n",
    "for epoch in range(FINE_TUNE_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{FINE_TUNE_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, pres_train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_results = evaluate(model, pres_test_loader, device)\n",
    "    print(f\"Test Loss: {val_results['loss']:.4f}\")\n",
    "    print(f\"Test F1 (micro): {val_results['f1_micro']:.4f}\")\n",
    "    print(f\"Test F1 (macro): {val_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    if val_results['f1_macro'] > best_pres_f1:\n",
    "        best_pres_f1 = val_results['f1_macro']\n",
    "        torch.save(model.state_dict(), 'best_presidential_modernbert_model.pt')\n",
    "        print(\"Saved best presidential model!\")\n",
    "\n",
    "print(f\"\\nBest Presidential Test F1 (macro): {best_pres_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289cc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL PRESIDENTIAL MODERNBERT MODEL RESULTS\n",
      "============================================================\n",
      "Test Loss: 0.0488\n",
      "Test F1 (micro): 0.7202\n",
      "Test F1 (macro): 0.0526\n",
      "\n",
      "Detailed Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.00      0.00      0.00         3\n",
      "     amusement       0.00      0.00      0.00         0\n",
      "         anger       0.00      0.00      0.00         0\n",
      "     annoyance       0.00      0.00      0.00         0\n",
      "      approval       0.57      0.11      0.19        36\n",
      "        caring       0.00      0.00      0.00         1\n",
      "     confusion       0.00      0.00      0.00         0\n",
      "     curiosity       0.00      0.00      0.00         1\n",
      "        desire       0.00      0.00      0.00         2\n",
      "disappointment       0.00      0.00      0.00         0\n",
      "   disapproval       0.00      0.00      0.00         0\n",
      "       disgust       0.00      0.00      0.00         0\n",
      " embarrassment       0.00      0.00      0.00         0\n",
      "    excitement       0.00      0.00      0.00         0\n",
      "          fear       0.00      0.00      0.00         0\n",
      "     gratitude       1.00      0.17      0.29        12\n",
      "         grief       0.00      0.00      0.00         0\n",
      "           joy       0.00      0.00      0.00         0\n",
      "          love       0.00      0.00      0.00         0\n",
      "   nervousness       0.00      0.00      0.00         0\n",
      "      optimism       0.50      0.08      0.14        12\n",
      "         pride       0.00      0.00      0.00         0\n",
      "   realization       0.00      0.00      0.00         0\n",
      "        relief       0.00      0.00      0.00         0\n",
      "       remorse       0.00      0.00      0.00         0\n",
      "       sadness       0.00      0.00      0.00         0\n",
      "      surprise       0.00      0.00      0.00         0\n",
      "       neutral       0.83      0.89      0.86       149\n",
      "\n",
      "     micro avg       0.82      0.64      0.72       216\n",
      "     macro avg       0.10      0.04      0.05       216\n",
      "  weighted avg       0.75      0.64      0.65       216\n",
      "   samples avg       0.70      0.64      0.66       216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/xiang/miniconda3/envs/modernbert_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Load best presidential model\n",
    "model.load_state_dict(torch.load('best_presidential_modernbert_model.pt'))\n",
    "\n",
    "final_results = evaluate(model, pres_test_loader, device)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL PRESIDENTIAL MODERNBERT MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {final_results['loss']:.4f}\")\n",
    "print(f\"Test F1 (micro): {final_results['f1_micro']:.4f}\")\n",
    "print(f\"Test F1 (macro): {final_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    final_results['labels'], \n",
    "    final_results['predictions'], \n",
    "    target_names=LABELS,\n",
    "    zero_division=0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modernbert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
