{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74102221",
   "metadata": {},
   "source": [
    "# RoBERTa Emotion Classification\n",
    "This notebook trains a RoBERTa model on the GoEmotions dataset and then fine-tunes it on the presidential speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75cb8d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (1.8.0)\n",
      "Requirement already satisfied: protobuf in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (6.33.2)\n",
      "Requirement already satisfied: filelock in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from accelerate) (2.9.1+cu130)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: setuptools in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install transformers datasets accelerate scikit-learn protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5b4eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe889c",
   "metadata": {},
   "source": [
    "### Load Data and Define Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc3dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 43410, Val: 5426, Test: 5427\n"
     ]
    }
   ],
   "source": [
    "# Load the GoEmotions dataset\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_LABELS = len(LABELS)\n",
    "\n",
    "def convert_to_df(split):\n",
    "    data = dataset[split]\n",
    "    rows = []\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]['text']\n",
    "        label_ids = data[i]['labels']\n",
    "        label_vec = [1 if j in label_ids else 0 for j in range(NUM_LABELS)]\n",
    "        rows.append([text] + label_vec)\n",
    "    return pd.DataFrame(rows, columns=['text'] + LABELS)\n",
    "\n",
    "train_df = convert_to_df('train')\n",
    "val_df = convert_to_df('validation')\n",
    "test_df = convert_to_df('test')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029da784",
   "metadata": {},
   "source": [
    "### Initialize RoBERTa Tokenizer and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a06c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RoBERTa base tokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = LABELS\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.loc[idx, 'text'])\n",
    "        labels = self.data.loc[idx, self.labels].values.astype(float)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = EmotionDataset(train_df, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada38ae3",
   "metadata": {},
   "source": [
    "### Initialize RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafc50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model roberta-base loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Model {MODEL_NAME} loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867269b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs.logits)\n",
    "            preds = (probs > threshold).float()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c6c93",
   "metadata": {},
   "source": [
    "### Train RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2085cc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [03:26<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:09<00:00, 37.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0915\n",
      "Validation F1 (micro): 0.5325\n",
      "Validation F1 (macro): 0.2999\n",
      "Saved best model!\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [03:14<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:09<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0836\n",
      "Validation F1 (micro): 0.5742\n",
      "Validation F1 (macro): 0.3910\n",
      "Saved best model!\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [03:17<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:08<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0828\n",
      "Validation F1 (micro): 0.5826\n",
      "Validation F1 (macro): 0.4420\n",
      "Saved best model!\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2714/2714 [03:08<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:08<00:00, 40.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0838\n",
      "Validation F1 (micro): 0.5829\n",
      "Validation F1 (macro): 0.4523\n",
      "Saved best model!\n",
      "\n",
      "Best Validation F1 (macro): 0.4523\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_results = evaluate(model, val_loader, device)\n",
    "    print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "    print(f\"Validation F1 (micro): {val_results['f1_micro']:.4f}\")\n",
    "    print(f\"Validation F1 (macro): {val_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    if val_results['f1_macro'] > best_f1:\n",
    "        best_f1 = val_results['f1_macro']\n",
    "        torch.save(model.state_dict(), 'best_roberta_model.pt')\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "print(f\"\\nBest Validation F1 (macro): {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008f2db",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad67530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 340/340 [00:09<00:00, 37.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ROBERTA TEST RESULTS\n",
      "==================================================\n",
      "Test F1 (micro): 0.5872\n",
      "Test F1 (macro): 0.4503\n",
      "\n",
      "Detailed Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.70      0.73      0.72       504\n",
      "     amusement       0.78      0.88      0.83       264\n",
      "         anger       0.55      0.45      0.50       198\n",
      "     annoyance       0.50      0.23      0.31       320\n",
      "      approval       0.56      0.35      0.43       351\n",
      "        caring       0.53      0.36      0.43       135\n",
      "     confusion       0.56      0.34      0.42       153\n",
      "     curiosity       0.55      0.45      0.49       284\n",
      "        desire       0.70      0.42      0.53        83\n",
      "disappointment       0.62      0.12      0.20       151\n",
      "   disapproval       0.49      0.32      0.39       267\n",
      "       disgust       0.65      0.33      0.44       123\n",
      " embarrassment       0.73      0.30      0.42        37\n",
      "    excitement       0.54      0.33      0.41       103\n",
      "          fear       0.68      0.71      0.69        78\n",
      "     gratitude       0.95      0.90      0.93       352\n",
      "         grief       0.00      0.00      0.00         6\n",
      "           joy       0.71      0.58      0.64       161\n",
      "          love       0.76      0.85      0.81       238\n",
      "   nervousness       0.00      0.00      0.00        23\n",
      "      optimism       0.64      0.48      0.55       186\n",
      "         pride       0.00      0.00      0.00        16\n",
      "   realization       0.48      0.09      0.15       145\n",
      "        relief       0.00      0.00      0.00        11\n",
      "       remorse       0.60      0.71      0.65        56\n",
      "       sadness       0.61      0.48      0.54       156\n",
      "      surprise       0.61      0.45      0.52       141\n",
      "       neutral       0.75      0.53      0.62      1787\n",
      "\n",
      "     micro avg       0.69      0.51      0.59      6329\n",
      "     macro avg       0.55      0.41      0.45      6329\n",
      "  weighted avg       0.66      0.51      0.57      6329\n",
      "   samples avg       0.57      0.54      0.55      6329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_roberta_model.pt'))\n",
    "test_results = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ROBERTA TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test F1 (micro): {test_results['f1_micro']:.4f}\")\n",
    "print(f\"Test F1 (macro): {test_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    test_results['labels'], \n",
    "    test_results['predictions'], \n",
    "    target_names=LABELS,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e0141",
   "metadata": {},
   "source": [
    "## Domain Adaptation: Presidential Speeches\n",
    "Fine-tune the best RoBERTa model on the presidential speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76165763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presidential Dataset Shape: (995, 67)\n",
      "Using text column: speech\n"
     ]
    }
   ],
   "source": [
    "# Load Presidential Data\n",
    "pres_df = pd.read_csv(\"data/presidential_speeches_goemotions_labeled.csv\")\n",
    "print(f\"Presidential Dataset Shape: {pres_df.shape}\")\n",
    "\n",
    "# Identify text column\n",
    "text_col = None\n",
    "for col in ['speech', 'Speech', 'transcript', 'Transcript', 'text', 'Text', 'content']:\n",
    "    if col in pres_df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    raise ValueError(f\"Could not find text column. Available columns: {pres_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using text column: {text_col}\")\n",
    "\n",
    "# Train-test split\n",
    "pres_train_df, pres_test_df = train_test_split(pres_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Dataset class for Presidential data\n",
    "class PresidentialDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, text_col, max_length=256):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_col = text_col\n",
    "        self.labels = LABELS\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.loc[idx, self.text_col])\n",
    "        if pd.isna(text):\n",
    "            text = \"\"\n",
    "        labels = self.data.loc[idx, self.labels].values.astype(float)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "pres_train_dataset = PresidentialDataset(pres_train_df, tokenizer, text_col)\n",
    "pres_test_dataset = PresidentialDataset(pres_test_df, tokenizer, text_col)\n",
    "\n",
    "pres_train_loader = DataLoader(pres_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "pres_test_loader = DataLoader(pres_test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951183e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning RoBERTa on presidential data for 3 epochs...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:09<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  4.61it/s]\n",
      "/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0557\n",
      "Test F1 (micro): 0.7128\n",
      "Test F1 (macro): 0.0379\n",
      "Saved best presidential model!\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:09<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:01<00:00,  6.76it/s]\n",
      "/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0553\n",
      "Test F1 (micro): 0.6775\n",
      "Test F1 (macro): 0.0389\n",
      "Saved best presidential model!\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:09<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:01<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0557\n",
      "Test F1 (micro): 0.6667\n",
      "Test F1 (macro): 0.0385\n",
      "\n",
      "Best Presidential Test F1 (macro): 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Load the best RoBERTa model\n",
    "model.load_state_dict(torch.load('best_roberta_model.pt'))\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "FINE_TUNE_LR = 1e-5\n",
    "FINE_TUNE_EPOCHS = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=FINE_TUNE_LR, weight_decay=0.01)\n",
    "total_steps = len(pres_train_loader) * FINE_TUNE_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "print(f\"Fine-tuning RoBERTa on presidential data for {FINE_TUNE_EPOCHS} epochs...\")\n",
    "\n",
    "best_pres_f1 = 0\n",
    "\n",
    "for epoch in range(FINE_TUNE_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{FINE_TUNE_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, pres_train_loader, optimizer, scheduler, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_results = evaluate(model, pres_test_loader, device)\n",
    "    print(f\"Test Loss: {val_results['loss']:.4f}\")\n",
    "    print(f\"Test F1 (micro): {val_results['f1_micro']:.4f}\")\n",
    "    print(f\"Test F1 (macro): {val_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    if val_results['f1_macro'] > best_pres_f1:\n",
    "        best_pres_f1 = val_results['f1_macro']\n",
    "        torch.save(model.state_dict(), 'best_presidential_roberta_model.pt')\n",
    "        print(\"Saved best presidential model!\")\n",
    "\n",
    "print(f\"\\nBest Presidential Test F1 (macro): {best_pres_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e692de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL PRESIDENTIAL ROBERTA MODEL RESULTS\n",
      "============================================================\n",
      "Test Loss: 0.0553\n",
      "Test F1 (micro): 0.6775\n",
      "Test F1 (macro): 0.0389\n",
      "\n",
      "Detailed Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.00      0.00      0.00         3\n",
      "     amusement       0.00      0.00      0.00         0\n",
      "         anger       0.00      0.00      0.00         0\n",
      "     annoyance       0.00      0.00      0.00         0\n",
      "      approval       0.00      0.00      0.00        36\n",
      "        caring       0.00      0.00      0.00         1\n",
      "     confusion       0.00      0.00      0.00         0\n",
      "     curiosity       0.00      0.00      0.00         1\n",
      "        desire       0.00      0.00      0.00         2\n",
      "disappointment       0.00      0.00      0.00         0\n",
      "   disapproval       0.00      0.00      0.00         0\n",
      "       disgust       0.00      0.00      0.00         0\n",
      " embarrassment       0.00      0.00      0.00         0\n",
      "    excitement       0.00      0.00      0.00         0\n",
      "          fear       0.00      0.00      0.00         0\n",
      "     gratitude       0.67      0.17      0.27        12\n",
      "         grief       0.00      0.00      0.00         0\n",
      "           joy       0.00      0.00      0.00         0\n",
      "          love       0.00      0.00      0.00         0\n",
      "   nervousness       0.00      0.00      0.00         0\n",
      "      optimism       0.00      0.00      0.00        12\n",
      "         pride       0.00      0.00      0.00         0\n",
      "   realization       0.00      0.00      0.00         0\n",
      "        relief       0.00      0.00      0.00         0\n",
      "       remorse       0.00      0.00      0.00         0\n",
      "       sadness       0.00      0.00      0.00         0\n",
      "      surprise       0.00      0.00      0.00         0\n",
      "       neutral       0.82      0.83      0.82       149\n",
      "\n",
      "     micro avg       0.82      0.58      0.68       216\n",
      "     macro avg       0.05      0.04      0.04       216\n",
      "  weighted avg       0.60      0.58      0.58       216\n",
      "   samples avg       0.63      0.58      0.59       216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/xiang/miniconda3/envs/nlp/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Load best presidential model\n",
    "model.load_state_dict(torch.load('best_presidential_roberta_model.pt'))\n",
    "\n",
    "final_results = evaluate(model, pres_test_loader, device)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL PRESIDENTIAL ROBERTA MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {final_results['loss']:.4f}\")\n",
    "print(f\"Test F1 (micro): {final_results['f1_micro']:.4f}\")\n",
    "print(f\"Test F1 (macro): {final_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    final_results['labels'], \n",
    "    final_results['predictions'], \n",
    "    target_names=LABELS,\n",
    "    zero_division=0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
